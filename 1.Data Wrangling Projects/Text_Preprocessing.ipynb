{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2_31258301",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fLq6IYZmkS-"
      },
      "source": [
        "# FIT5196 Assessment 1 Task 2\n",
        "#### Student Name: Pichaphop Sunthornjittanon\n",
        "#### Student ID: 31258301\n",
        "\n",
        "Date: 31/8/21\n",
        " ##############################\n",
        "Version: 2.0\n",
        "\n",
        "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
        "\n",
        "## 1. Introduction\n",
        "This task touches on the next step of analyzing textual data, i.e., converting the extracted data\n",
        "into a numeric representation. In this task, you are required to write Python code to preprocess a\n",
        "set of articles about cryptocurrency and convert them into numerical representations (which are\n",
        "suitable for input into recommender-systems/ information-retrieval algorithms).\n",
        "\n",
        "Your task is to extract and transform the information of the PDF file performing the following task:\n",
        "1. Generate the corpus vocabulary with the same structure as sample_vocab.txt. Please\n",
        "note that the vocabulary must be sorted alphabetically.\n",
        "2. For each day (articles come with a date at their title), generate the sparse representation\n",
        "(i.e., doc-term matrix) of the PDF file according to the structure of the\n",
        "sample_countVec.txt. The articles of the same date must be concatenated before\n",
        "converting to the vector representation. The order of concatenation is not important\n",
        "for us (e.g., assuming “article1” and “article2” are both written on the same day, then you\n",
        "can either do article1+article2 or article2+article1).\n",
        "\n",
        "The following steps must be performed (not necessarily in the same order) to complete the\n",
        "assessment.please note that the order of preprocessing matters and will result in different\n",
        "vocabulary and hence different count vectors. It is part of the assessment to figure out the\n",
        "correct order of preprocessing which makes the most sense as we learned in the tutorials. If in\n",
        "doubt, you are encouraged to ask questions and discuss with the teaching team.\n",
        "1. The word tokenization must use the following regular expression,\n",
        "\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
        "2. The context-independent and context-dependent stopwords must be removed from\n",
        "the vocabulary.\n",
        "○ For context-independent, The provided context-independent stop words list (i.e,\n",
        "stopwords_en.txt) must be used.\n",
        "○ For context-dependent stopwords, you must set the threshold to more than\n",
        "ceil(Number_of_days / 2).\n",
        "3. Tokens should be stemmed using the Porter stemmer.\n",
        "4. Rare tokens (with the threshold set to less than 10 days) must be removed from the\n",
        "vocab.\n",
        "5. Creating the sparse matrix using countvectorizer.\n",
        "6. Tokens with a length less than 3 should be removed from the vocab.\n",
        "7. First 200 meaningful bigrams (i.e., collocations) must be included in the vocab using\n",
        "PMI measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2YuBKO9nkuQ"
      },
      "source": [
        "## 2.  Importing libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-3O7lsXodu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8612d6bc-a26e-47ea-b612-223b4eec8bac"
      },
      "source": [
        "# Download pdfminer\n",
        "!pip install pdfminer.six==20181108\n",
        "\n",
        "\n",
        "# Use for regular expression\n",
        "import re\n",
        "\n",
        "# Use for import pdf file\n",
        "from io import StringIO\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "\n",
        "# Use for text preprocessing\n",
        "import nltk \n",
        "\n",
        "# Use for tokenisation using regular expression \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Use for porter stemming \n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Use for calculate term and document frequency\n",
        "from nltk.probability import *\n",
        "\n",
        "# Use for retokenization after considering bigrams\n",
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "# Use for math operation such as ceil\n",
        "import math\n",
        "from __future__ import division\n",
        "\n",
        "# Use the iterator to join all the words together\n",
        "from itertools import chain\n",
        "import itertools\n",
        "\n",
        "# Use for create sparse matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfminer.six==20181108 in /usr/local/lib/python3.7/dist-packages (20181108)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20181108) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20181108) (1.15.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20181108) (3.10.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWBVZhnbnzKQ"
      },
      "source": [
        "## 3. Examining and loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_5TBdpJzz1n"
      },
      "source": [
        "In this section, we loaded and roughly explored the file from google drive with the file name 31258301_task2_pdf.pdf, which will be further processed. We used pdf Miner to transform PDF to raw text and store the text data into raw text varaible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqkgSnGen1RO",
        "outputId": "28587976-3816-4a5f-c025-913819455dad"
      },
      "source": [
        "# Mount Google Drive in the colab environment\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xTzqCSAzqWe"
      },
      "source": [
        "# Data from Task2 datasets/31258301_task2_pdf\n",
        "# Import PDF File\n",
        "# Reference : https://pdfminersix.readthedocs.io/en/latest/tutorial/composable.html\n",
        "\n",
        "output_string = StringIO()\n",
        "with open('/content/drive/Shareddrives/FIT5196-s2-2021-tutorials/Assessment 1/Task2 datasets/31258301_task2_pdf.pdf', 'rb') as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    for page in PDFPage.create_pages(doc):\n",
        "        interpreter.process_page(page)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSgOlp3r0Il3"
      },
      "source": [
        "# Store the raw data from PDF in raw_text\n",
        "raw_text = output_string.getvalue()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Roqx-BKaCR"
      },
      "source": [
        "## 4.Creating dictiory that contains key as dates and value as each day article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d2AYWv90e6K"
      },
      "source": [
        "After we obtained the raw text, we broke down the text in dictionary data type, which keys are dates and values are the articles writen in the given dates (If there were multiple articles in one day, concatatenate multiple articles together)\n",
        "\n",
        "To create dictionary, we utilised RegexpTokenizer to use regular expression to extract dates for keys and used dates as seperater to extract articles for values.\n",
        "\n",
        "The regular expression for extract dates that we used is \\\\[\\d{4}-\\d{2}-\\d{2} since we found that most of the dates are in this format [2018-06-30]. However, some data had square blacket missing such as [2016-01-17 so we considered this case as well.\n",
        "\n",
        "For the articles, since we want to exclude the topics, the regular expression for seperator is slighly different, which is \\\\[\\d{4}-\\d{1,2}-\\d{1,2}(?:.*?\\n\\n)\n",
        "\n",
        "After that we cleaned the date data by striping ] and whitespace at the beginning and end of the artcle and replace newline to space. Finally, we concat the articles that were written the same day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M84OmnoRByx7"
      },
      "source": [
        "# Create list of Article, which will use for values in dictionary\n",
        "article_tokenizer = RegexpTokenizer(r\"\\[\\d{4}-\\d{1,2}-\\d{1,2}(?:.*?\\n\\n)\", gaps=True,flags= re.DOTALL )\n",
        "article = article_tokenizer.tokenize(raw_text)\n",
        "\n",
        "# Create list of Date, which will use for key in dictionary\n",
        "date_tokenizer = RegexpTokenizer(r\"\\[\\d{4}-\\d{1,2}-\\d{1,2}\", gaps=False)\n",
        "dates = date_tokenizer.tokenize(raw_text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFUtWk1hAQTq"
      },
      "source": [
        "# # Clean the date data\n",
        "dates = [w.strip('[') for w in dates] \n",
        "dates_split = [date.split('-') for date in dates] \n",
        "dates = [date[0]+'-'+date[1].zfill(2)+'-'+date[2].zfill(2) for date in dates_split]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij5FpFE2Eadf"
      },
      "source": [
        "# Create dictionary and concat articles if there were more than one article a day \n",
        "daily_article = {}\n",
        "for index in range(0,len(dates)):\n",
        "\n",
        "  if dates[index] not in daily_article:\n",
        "    daily_article[dates[index]] = article[index]\n",
        "\n",
        "  else:\n",
        "    daily_article[dates[index]] = daily_article[dates[index]] + article[index]\n",
        "\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSNHmMSPP44c"
      },
      "source": [
        "##5.Tokenizing the Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tpJV_s37Ef7"
      },
      "source": [
        "In this section, we define the function that converts the articles to lower case and do tokenization breaking words into list by RegexpTokenizer with specified rule given in the specification. Then, we apply the function into the preprocessed data from the previous section and store in dictionary format name tokenised_article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR1bky5B5BSl"
      },
      "source": [
        "# Define the function used for tokenisation\n",
        "def tokenizeRawData(date):\n",
        "    \"\"\"\n",
        "        This function tokenizes a raw text document.\n",
        "    \"\"\"\n",
        "    # Convert text to lower case\n",
        "    raw_article = daily_article[date].lower()\n",
        "\n",
        "    # Define tokenisation rules\n",
        "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\", gaps=False)\n",
        "\n",
        "    # Tokenise the text\n",
        "    tokenised_article = tokenizer.tokenize(raw_article)\n",
        "\n",
        "    # Return tuple\n",
        "    return (date,tokenised_article) \n",
        "\n",
        "# Create dict of daily article using date as keys\n",
        "tokenised_article = dict(tokenizeRawData(date) for date in daily_article.keys())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS3yZlIWPJGW",
        "outputId": "dd6ce50b-e0ef-46bd-c129-eef49d33735b"
      },
      "source": [
        "# Check the size of vocab and token size\n",
        "words_check = list(chain.from_iterable(tokenised_article.values()))\n",
        "vocab_check = set(words_check)\n",
        "lexical_diversity = len(words_check)/len(vocab_check)\n",
        "print (\"Vocabulary size: \",len(vocab_check),\"\\nTotal number of tokens: \", len(words_check), \\\n",
        "\"\\nLexical diversity: \", lexical_diversity)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  10890 \n",
            "Total number of tokens:  89302 \n",
            "Lexical diversity:  8.200367309458219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc2K-c2I7Pa7"
      },
      "source": [
        "##6.Generate 200 meaningful Bigrams (Collocations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVaugyBETnDS"
      },
      "source": [
        "In this part,we want to generate 200 bigram collocation, given the tokenized articles above.\n",
        "\n",
        "By doing this, we started from concatenating all words by using chain.from_iterable. Then, BigramAssocMeasures(), BigramCollocationFinder.from_words(all_words) and bigram_finder.nbest(bigram_measures.pmi, 200)  are applied to find 200 meaningful bigrams using PMI measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBXUYjsb8LW4"
      },
      "source": [
        "# Create list of all words\n",
        "all_words = list(chain.from_iterable(tokenised_article.values()))\n",
        "\n",
        "# Find top 200 bigram using PMI\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
        "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNiXnXWjVew8"
      },
      "source": [
        "Next,we use MWETokenizer to tokenize the text again without spliting the words, which are belong in the 200 bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gOmu6fTRYY3"
      },
      "source": [
        "# Combine bigram to the tokens\n",
        "mwetokenizer = MWETokenizer(top_200_bigrams)\n",
        "tokenised_colloc_articles =  dict((date, mwetokenizer.tokenize(article)) for date,article in tokenised_article.items())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3OmSi7EgDJs",
        "outputId": "d69dce50-82c2-4dce-9769-af8100c5f8da"
      },
      "source": [
        "# Check the size of vocab and token size\n",
        "\n",
        "words_check = list(chain.from_iterable(tokenised_colloc_articles.values()))\n",
        "vocab_check = set(words_check)\n",
        "lexical_diversity = len(words_check)/len(vocab_check)\n",
        "print (\"Vocabulary size: \",len(vocab_check),\"\\nTotal number of tokens: \", len(words_check), \\\n",
        "\"\\nLexical diversity: \", lexical_diversity)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  10706 \n",
            "Total number of tokens:  89118 \n",
            "Lexical diversity:  8.324117317392117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT1499QTQOqz"
      },
      "source": [
        "##7.Removing  Context-Independent Stopwords  & Tokens with Length Less than 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMZSlZKZdV_"
      },
      "source": [
        "In this part, we remove context independent stopwords from our vocabularies since they are the words that are commonly found in most of the documents. By removing the stopwords, we download the given list of stopwords in the stopwords_en.txt and filter those out along with the words that have length less than 3 (Given in specification) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qek-cRZM96oD"
      },
      "source": [
        "# Download the given context independent stopwords file\n",
        "with open('/content/drive/Shareddrives/FIT5196-s2-2021-tutorials/Assessment 1/Task2 datasets/stopwords_en.txt','r') as infile:\n",
        "    context_ind_stopwords = infile.read().splitlines()\n",
        "\n",
        "# Convert into set of context_ind_stopwords\n",
        "context_ind_stopwords = set(context_ind_stopwords)\n",
        "\n",
        "# Initialise new dictionary\n",
        "tokenised_colloc_nonstop_articles ={}\n",
        "\n",
        "# Remove context independent stop word and tokens that have length less than 3\n",
        "for key in tokenised_colloc_articles.keys() :\n",
        "  tokenised_colloc_nonstop_articles[key] = [w for w in tokenised_colloc_articles[key] if w not in context_ind_stopwords]\n",
        "  tokenised_colloc_nonstop_articles[key] = [w for w in tokenised_colloc_nonstop_articles[key] if len(w)>=3]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CrBlnRSPdDC",
        "outputId": "97059d59-3ac4-498e-d458-4d014d49c167"
      },
      "source": [
        "# Check the size of vocab and token size\n",
        "words_check = list(chain.from_iterable(tokenised_colloc_nonstop_articles.values()))\n",
        "vocab_check = set(words_check)\n",
        "lexical_diversity = len(words_check)/len(vocab_check)\n",
        "print (\"Vocabulary size: \",len(vocab_check),\"\\nTotal number of tokens: \", len(words_check), \\\n",
        "\"\\nLexical diversity: \", lexical_diversity)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  10140 \n",
            "Total number of tokens:  46313 \n",
            "Lexical diversity:  4.567357001972387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmOWabWKY8EC"
      },
      "source": [
        "##8.Removing Context-Dependent Stopwords and Rare Tokens (Less than 10 days) in Unigram\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R6oleOCYSsw"
      },
      "source": [
        "In the requirement, we are asked to exclude Context-Dependent Stopwords and Rare Tokens (Less than 10 days), but include 200 meaningful bigrams. We assume that by removing the stopwords and rare token, it is done in unigrams.\n",
        "\n",
        "In the first step, we create the words list shown in the code and apply FreqDist function to obtain document frequency in each word.\n",
        "\n",
        "Then, we use document frequency values to filter the words that are in context-dependent stopwords (doc freq more than ceil(number of days/2)) and rare token(doc freq less than 10) conditions keeping the bigrams in the vocabularies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHqQYLLgZCWq"
      },
      "source": [
        "# List of each document unique word\n",
        "words = list(chain.from_iterable([set(value) for value in tokenised_colloc_nonstop_articles.values()]))\n",
        "\n",
        "# Find the document frequency in each word\n",
        "doc_freq = FreqDist(words)\n",
        "\n",
        "# Create the list of context dependent stopwords that we want to remove\n",
        "context_dependent = [word for word,freq in doc_freq.items() if freq > math.ceil(len(tokenised_colloc_nonstop_articles)/2)]\n",
        "context_dependent = set(context_dependent)\n",
        "\n",
        "# Create the list of rare word that we want to remove\n",
        "rare_word = [word for word,freq in doc_freq.items() if freq < 10]\n",
        "rare_word = set(rare_word)\n",
        "\n",
        "# Create the list of 200 bigrams obtained above\n",
        "top_200_bigrams_list = ['_'.join(word) for word in top_200_bigrams]\n",
        "top_200_bigrams_set = set(top_200_bigrams_list)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msv01dnZa1wi"
      },
      "source": [
        "# Create set of context dependent stopword and rare word that we want to remove from the vocab (exlude bigram)\n",
        "remove_context_dependent = context_dependent-top_200_bigrams_set\n",
        "remove_rare_word = rare_word-top_200_bigrams_set"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34a0UtJ8cLPC"
      },
      "source": [
        "# Initialise the new dict\n",
        "tokenised_colloc_nonstop_removedocfreq_articles = {}\n",
        "\n",
        "# Remove Context-Dependent Stopwords and Rare Tokens\n",
        "for key in tokenised_colloc_nonstop_articles.keys() :\n",
        "  tokenised_colloc_nonstop_removedocfreq_articles[key] = [w for w in tokenised_colloc_nonstop_articles[key] if w not in remove_context_dependent]\n",
        "  tokenised_colloc_nonstop_removedocfreq_articles[key] = [w for w in tokenised_colloc_nonstop_removedocfreq_articles[key] \n",
        "                                                                       if w not in remove_rare_word]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d8prtsna1yt",
        "outputId": "efcdc583-c708-4cda-82ba-6f610693c8ac"
      },
      "source": [
        "# Check the size of vocab and token size\n",
        "words_check = list(chain.from_iterable(tokenised_colloc_nonstop_removedocfreq_articles.values()))\n",
        "vocab_check = set(words_check)\n",
        "lexical_diversity = len(words_check)/len(vocab_check)\n",
        "print (\"Vocabulary size: \",len(vocab_check),\"\\nTotal number of tokens: \", len(words_check), \\\n",
        "\"\\nLexical diversity: \", lexical_diversity)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  904 \n",
            "Total number of tokens:  23481 \n",
            "Lexical diversity:  25.974557522123895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FdryLtSeMlj"
      },
      "source": [
        "##9.Stemming Unigram Using Porter Stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-2lvA2HexZp"
      },
      "source": [
        "In this part, we use porter stemming algorithm to stem the unigram in order to group words with similar meaning but different forms together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqZqxYdQeVQb"
      },
      "source": [
        "# Use Porter Stemming algorithm\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Initialise the new dict\n",
        "tokenised_colloc_nonstop_removedocfreq_stem_articles = {}\n",
        "\n",
        "# Do stemming in unigram\n",
        "for date,words in tokenised_colloc_nonstop_removedocfreq_articles.items() :\n",
        "  tokenised_colloc_nonstop_removedocfreq_stem_articles[date] = [stemmer.stem(w) if w not in top_200_bigrams_set else w for w in words  ]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x42R7P0Mj7gv",
        "outputId": "b4ff9c2d-d42f-402e-86a5-00842f67885c"
      },
      "source": [
        "# Check the size of vocab and token size\n",
        "words_check = list(chain.from_iterable(tokenised_colloc_nonstop_removedocfreq_stem_articles.values()))\n",
        "vocab_check = set(words_check)\n",
        "lexical_diversity = len(words_check)/len(vocab_check)\n",
        "print (\"Vocabulary size: \",len(vocab_check),\"\\nTotal number of tokens: \", len(words_check), \\\n",
        "\"\\nLexical diversity: \", lexical_diversity)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  751 \n",
            "Total number of tokens:  23481 \n",
            "Lexical diversity:  31.26631158455393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxIdia8aSLXK"
      },
      "source": [
        "##10.Creating a sparse matrix using countvectrorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYtXYmoT63qb"
      },
      "source": [
        "Finally,we create sparse matrix using countveoctorizer and generate the final outputs, which are count vector for each date in 31258301_countVec.txt file and list of vocabulary in 31258301_vocab.txt file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEKfo1ZTjNf3",
        "outputId": "f99b86eb-8b6e-4c81-b5fa-bef3c9faebd6"
      },
      "source": [
        "# Define spliting function\n",
        "# Source : https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
        "def spliting(string):\n",
        "    return string.split()\n",
        "\n",
        "# Use CountVectorizer function \n",
        "vectorizer = CountVectorizer(tokenizer = spliting) \n",
        "\n",
        "# Create data_features using the fit_transform function \n",
        "data_features = vectorizer.fit_transform([' '.join(article) for date,article in tokenised_colloc_nonstop_removedocfreq_stem_articles.items()])\n",
        "print (data_features.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(394, 751)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANQaFk-BxHT1"
      },
      "source": [
        "# Write the 31258301_countVec.txt \n",
        "\n",
        "with open('31258301_countVec.txt','w') as file:\n",
        "\n",
        "    # Create coordinate dat format \n",
        "    cx = data_features.tocoo() \n",
        "\n",
        "    # Intialise this variable to determine whether it start a new line or not\n",
        "    var = -1\n",
        "\n",
        "    # Iterate through all coordiation in sparse matrix\n",
        "    for i,j,v in itertools.zip_longest(cx.row, cx.col, cx.data):\n",
        "\n",
        "        # If it changes the row, write a new line and new date\n",
        "        if var != i:\n",
        "            if (var != -1):\n",
        "                file.write(\"\\n\") \n",
        "            file.write(list(tokenised_colloc_nonstop_removedocfreq_stem_articles.keys())[i])\n",
        "\n",
        "            # Assign i to var\n",
        "            var = i\n",
        "        # Report the count vector of the indexed words\n",
        "        file.write(\",\"+str(j)+\":\"+str(v))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKA50JBbjTBK"
      },
      "source": [
        "# Write the 31258301_vocab.txt \n",
        "\n",
        "# Obtain vocabs\n",
        "vocab = vectorizer.get_feature_names()\n",
        "\n",
        "# Write vocab to output file\n",
        "with open('31258301_vocab.txt','w') as file:\n",
        "  for words in range (len(vocab)):\n",
        "    file.write(vocab[words]+\":\"+str(words)+\"\\n\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HKz4oH27gga"
      },
      "source": [
        "#Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy4KZglV7vnQ"
      },
      "source": [
        "In this assignment, we perform several steps to convert texts from the given PDF file to the list of vocabulary and count vectors, which are generated into the text files. The steps include :\n",
        "\n",
        "1. **Introduction** - Understand the requirement of this assignment\n",
        "2. **Importing libraries** -Import several libraries used for this task\n",
        "3. **Examining and loading data** - Load the data from the given PDF file \n",
        "4. **Creating dictiory that contains key as dates and value as each day article** - Convert the loaded text into dictionary, which keys represent dates and values represent each day article\n",
        "5.**Tokenizing the Text** - Convert all words into lower cases and tokenize the text into the lists in the dictionary created in previous step\n",
        "6. **Generate 200 meaningful Bigrams (Collocations)** - Find the first 200 meaningful bigrams using PMI measures and retokenize again using MWETokenizer.\n",
        "7. **Removing Context-Independent Stopwords & Tokens with Length Less than 3** - Remove the given context-independent stopwords and the words that have length less than 3 to our vocabularies.\n",
        "8.**Removing Context-Dependent Stopwords and Rare Tokens (Less than 10 days) in Unigram** - Remove the unigram that does not meet the given conditions, which are  in context-dependent stopwords (occur more than 50% of the days) and Rare Tokens (occur less than 10 days)\n",
        "9.**Stemming Unigram Using Porter Stemmer** - Do stemming for the unigram using porter stemmer\n",
        "10.**Creating a sparse matrix using countvectrorizer** - Create the sparse matrix using countvectorizer and generate final outputs, which are 31258301_countVec.txt and 31258301_vocab.txt"
      ]
    }
  ]
}