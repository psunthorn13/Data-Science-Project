{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 : Analysing Flight-delays Data\n",
    "\n",
    "Name : Pichaphop Sunthornjittanon\n",
    "\n",
    "Student ID : 31258301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents<a class=\"anchor\" id=\"table\"></a>\n",
    "\n",
    "* [1 Working with RDD](#1)\n",
    "* [1.1 Data Preparation and Loading](#1.1)\n",
    "* [1.1.1 Creating SparkSession & SparkContext](#OneOneOne)\n",
    "* [1.1.2 Read CSV files, Preprocessing, and final(formatted data) RDD for each file](#OneOneTwo)\n",
    "* [1.1.2.1 Flights RDD](#1.1.2.1)\n",
    "* [1.1.2.2 Airports RDD](#1.1.2.2)\n",
    "* [1.1.3 Show RDD number of columns, and number of records](#1.1.3)\n",
    "* [1.2 Dataset flights partitioning](#1.2)\n",
    "* [1.2.1 Obtain the maximum arrival time ](#1.2.1)\n",
    "* [1.2.2 Obtain the maximum minimum time ](#1.2.2)\n",
    "* [1.2.3 Define hash partitioning](#1.2.3)\n",
    "* [1.2.4 Display the records in each partition](#1.2.4)\n",
    "* [1.3 Query RDD](#1.3)\n",
    "* [1.3.1 Collect a total number of flights for each month for all flights](#1.3.1)\n",
    "* [1.3.2 Collect the average delay for each month for all flights](#1.3.2)\n",
    "* [2 Working with DataFrames](#2)\n",
    "* [2.1 Data Preparation and Loading](#2.1)\n",
    "* [2.1.1 Define DataFrames](#2.1.1)\n",
    "* [2.1.2 Display the Scheme of DataFrames](#2.1.2)\n",
    "* [2.1.3 Transform date-time and location column](#2.1.3)\n",
    "* [2.2.1 January Flights Events with ANC airport](#2.2.1)\n",
    "* [2.2.2 Average Arrival Delay From Origin to Destination](#2.2.2)\n",
    "* [2.2.3 Join Query with Airports DataFrame](#2.2.3)\n",
    "* [2.3 Analysis](#2.3.1)\n",
    "* [2.3.1 Relationship between day of week with mean arrival delay, total time delay, and count flights](#2.3.1)\n",
    "* [2.3.2 Display mean arrival delay each month](#2.3.2)\n",
    "* [2.3.3 Relationship between mean departure delay and mean arrival delay](#2.3.3)\n",
    "* [3 RDDs vs DataFrame vs Spark SQL](#3)\n",
    "* [3.1 RDD Operation](#3.1)\n",
    "* [3.2 DataFrame Operation](#3.1)\n",
    "* [3.3 Spark SQL Operation](#3.1)\n",
    "* [3.4 Discussion](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Working with RDD<a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "In this section, you will need to create RDDs from the given datasets, perform partitioning in\n",
    "these RDDs and use various RDD operations to answer the queries for crash analysis.\n",
    "\n",
    "## 1.1 Data Preparation and Loading<a class=\"anchor\" id=\"1.1\"></a>\n",
    "### 1.1.1 Create SparkSession and SparkContext<a class=\"anchor\" id=\"OneOneOne\"></a>\n",
    "[Back to top](#table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write the code to create a SparkContext object using SparkSession, which tells Spark\n",
    "how to access a cluster. To create a SparkSession you first need to build a SparkConf\n",
    "object that contains information about your application. Give an appropriate name for\n",
    "your application and run Spark locally with as many working processors as logical\n",
    "cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "master = \"local[*]\"\n",
    "\n",
    "\n",
    "# Assign the name to be shown on the Spark cluster UI page\n",
    "app_name = \"Analysing Flight Delay Data\"\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "#  Create SparkContext object using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Import CSV files and Make RDD for each file<a class=\"anchor\" id=\"OneOneTwo\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read the 20 files of “flight*.csv” file into a single RDD (flights_rdd) and\n",
    "“airports.csv” file into a single RDD (airports_rdd). For each RDD, remove the header\n",
    "row and parse each comma-delimited line into a Row object with each columnfollowing the data type given in the jupyter notebook cell. Please convert some\n",
    "columns into the preferred format. Columns that should be converted into integer :\n",
    "'YEAR', 'MONTH', 'DAY','DAY_OF_WEEK', 'FLIGHT_NUMBER'. Column that\n",
    "should be converted into float data type : 'DEPARTURE_DELAY',\n",
    "'ARRIVAL_DELAY', 'ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'TAXI_IN', and\n",
    "'TAXI_OUT'. While the rest are kept as string format. Note that in this preprocessing\n",
    "task, you are asked to build a set of functions which load the csv data into the RDD\n",
    "object, remove the header of the RDD object, and finally parse the RDD object into\n",
    "the desired format (integer, float, or string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.1 Flights RDD <a class=\"anchor\" id=\"1.1.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.1.2.1 Flights RDD\n",
    "\n",
    "# a.import all csv file that start with flight\n",
    "flight_rdd = sc.textFile('data/flight*.csv')\n",
    "\n",
    "# b. Split each line separated by comma into a list \n",
    "flight_rdd = flight_rdd.map(lambda line: line.split(','))\n",
    "\n",
    "# c. Remove the header\n",
    "flight_header = flight_rdd.first() \n",
    "flight_rdd = flight_rdd.filter(lambda x: x != flight_header)\n",
    "\n",
    "# d. Convert data type \n",
    "flight_rdd = flight_rdd.map(lambda x: [int(x[0] or 0),int(x[1] or 0),int(x[2] or 0),int(x[3] or 0),x[4],int(x[5] or 0),\n",
    "                                      x[6],x[7],x[8],x[9],x[10],float(x[11] or 0),\n",
    "                                      float(x[12]or 0),x[13],x[14],float(x[15]or 0),float(x[16]or 0),float(x[17]or 0),\n",
    "                                      x[18],float(x[19]or 0),x[20],x[21],float(x[22]or 0),x[23],\n",
    "                                      x[24],x[25],x[26],x[27],x[28],x[29],x[30]])\n",
    "\n",
    "\n",
    "# print(\"Number of record :\",flight_rdd.count())\n",
    "# flight_rdd.take(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.2 Airports RDD <a class=\"anchor\" id=\"1.1.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1.2.2 Airports RDD\n",
    "\n",
    "# a. import all csv file that start with airport\n",
    "airport_rdd = sc.textFile('data/airports.csv')\n",
    "\n",
    "# b. Split each line separated by comma into a list \n",
    "airport_rdd = airport_rdd.map(lambda line: line.split(','))\n",
    "\n",
    "# c. Remove the header\n",
    "airport_header = airport_rdd.first() \n",
    "airport_rdd = airport_rdd.filter(lambda x: x != airport_header)\n",
    "\n",
    "# print(\"Number of record :\",airport_rdd.count())\n",
    "# airport_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Show RDD number of columns, and number of records <a class=\"anchor\" id=\"1.1.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For each RDD, display the number of columns, the total number of records, and\n",
    "display the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Flight----------\n",
      "Number of records for flight : 582184\n",
      "Number of columns for flight : 31\n",
      "Number of partition : 20\n",
      "-------Airport----------\n",
      "Number of records for airport: 322\n",
      "Number of columns for airport : 7\n",
      "Number of partition : 2\n"
     ]
    }
   ],
   "source": [
    "print(\"-------Flight----------\")\n",
    "print(\"Number of records for flight :\",flight_rdd.count())\n",
    "print(\"Number of columns for flight :\", len(flight_header))\n",
    "print(\"Number of partition :\", flight_rdd.getNumPartitions())\n",
    "\n",
    "print(\"-------Airport----------\")\n",
    "print(\"Number of records for airport:\",airport_rdd.count())\n",
    "print(\"Number of columns for airport :\", len(airport_header))\n",
    "print(\"Number of partition :\", airport_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Partitioning <a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will need to analyse the RDD partitions.\n",
    "\n",
    "● How many partitions do the above RDDs have?\n",
    "\n",
    "● How is the data in these RDDs partitioned by default, when we do not explicitly specify any partitioning strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "\n",
    "#A Function to print the data items in each RDD\n",
    "\n",
    "def print_partitions(data):\n",
    "    if isinstance(data, RDD):\n",
    "        numPartitions = data.getNumPartitions()\n",
    "        partitions = data.glom().collect()\n",
    "    else:\n",
    "        numPartitions = data.rdd.getNumPartitions()\n",
    "        partitions = data.rdd.glom().collect()\n",
    "    \n",
    "    print(f\"####### NUMBER OF PARTITIONS: {numPartitions}\")\n",
    "    for index, partition in enumerate(partitions):\n",
    "        # show partition if it is not empty\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################## FLIGHT ##################\n",
      "####### NUMBER OF PARTITIONS: 20\n",
      "Partition 0: 31221 records\n",
      "Partition 1: 30803 records\n",
      "Partition 2: 28472 records\n",
      "Partition 3: 30863 records\n",
      "Partition 4: 28471 records\n",
      "Partition 5: 28532 records\n",
      "Partition 6: 27039 records\n",
      "Partition 7: 30535 records\n",
      "Partition 8: 27234 records\n",
      "Partition 9: 29452 records\n",
      "Partition 10: 28615 records\n",
      "Partition 11: 30102 records\n",
      "Partition 12: 28940 records\n",
      "Partition 13: 28081 records\n",
      "Partition 14: 29844 records\n",
      "Partition 15: 27545 records\n",
      "Partition 16: 30542 records\n",
      "Partition 17: 24994 records\n",
      "Partition 18: 31056 records\n",
      "Partition 19: 29843 records\n",
      "################## AIRPORT ##################\n",
      "####### NUMBER OF PARTITIONS: 2\n",
      "Partition 0: 162 records\n",
      "Partition 1: 160 records\n"
     ]
    }
   ],
   "source": [
    "print('################## FLIGHT ##################')\n",
    "print_partitions(flight_rdd)\n",
    "\n",
    "print('################## AIRPORT ##################')\n",
    "print_partitions(airport_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "- How many partitions do the above RDDs have?\n",
    "\n",
    "flight_rdd has 20 partitions and airport_rdd has 2 partition\n",
    "\n",
    "- How is the data in these RDDs partitioned by default, when we do not explicitly specify any partitioning strategy?\n",
    "\n",
    "By deafault, Spark uses Random Equal Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once flights_rdd is created in 1.1.2, we note that the ‘ARRIVAL_DELAY’ column has been\n",
    "converted into a float data type. This column represents the gap between arrival time and the\n",
    "scheduled time represented in the column 'ARRIVAL_TIME' and 'SCHEDULED_ARRIVAL'\n",
    "respectively. Negative value means that the arrival time is earlier than scheduled time and\n",
    "vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Obtain the maximum arrival time <a class=\"anchor\" id=\"1.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Obtain the maximum arrival delay using RDD from flights_rdd (could be a positive\n",
    "value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum arrival delay is 1665.0\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum arrival delay\n",
    "print('The maximum arrival delay is',flight_rdd.max(key = lambda x: x[22])[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Obtain the minimum arrival time <a class=\"anchor\" id=\"1.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtain the minimum arrival delay using RDD from flights_rdd (could be a negative\n",
    "value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum arrival delay is -82.0\n"
     ]
    }
   ],
   "source": [
    "# Print the minimum arrival delay\n",
    "print('The minimum arrival delay is',flight_rdd.min(key = lambda x: x[22])[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Define hash partitioning function <a class=\"anchor\" id=\"1.2.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a python function and define a hash partitioning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hash function\n",
    "def hash_function(key):\n",
    "    total = 0\n",
    "    key = abs(int(key))\n",
    "    for digit in str(key):\n",
    "        total += int(digit)\n",
    "    return total\n",
    "\n",
    "no_of_partitions=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to include key(arrival delay) used for hash partioning  \n",
    "flight_hash_rdd = flight_rdd.map(lambda x: (x[22],[int(x[0] or 0),int(x[1] or 0),int(x[2] or 0),int(x[3] or 0),x[4],int(x[5] or 0),\n",
    "                                      x[6],x[7],x[8],x[9],x[10],float(x[11] or 0),\n",
    "                                      float(x[12]or 0),x[13],x[14],float(x[15]or 0),float(x[16]or 0),float(x[17]or 0),\n",
    "                                      x[18],float(x[19]or 0),x[20],x[21],float(x[22]or 0),x[23],\n",
    "                                      x[24],x[25],x[26],x[27],x[28],x[29],x[30]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hash_partitioned_rdd used hash partioning defined above\n",
    "hash_partitioned_rdd = flight_hash_rdd.partitionBy(no_of_partitions, hash_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Display the records in each partition <a class=\"anchor\" id=\"1.2.4\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NUMBER OF PARTITIONS: 20\n",
      "Partition 0: 23150 records\n",
      "Partition 1: 47995 records\n",
      "Partition 2: 59849 records\n",
      "Partition 3: 61278 records\n",
      "Partition 4: 61046 records\n",
      "Partition 5: 59210 records\n",
      "Partition 6: 56957 records\n",
      "Partition 7: 55308 records\n",
      "Partition 8: 53121 records\n",
      "Partition 9: 50535 records\n",
      "Partition 10: 24699 records\n",
      "Partition 11: 10902 records\n",
      "Partition 12: 6160 records\n",
      "Partition 13: 4023 records\n",
      "Partition 14: 2955 records\n",
      "Partition 15: 2107 records\n",
      "Partition 16: 1472 records\n",
      "Partition 17: 881 records\n",
      "Partition 18: 434 records\n",
      "Partition 19: 102 records\n"
     ]
    }
   ],
   "source": [
    "# Display the records in each partition\n",
    "print_partitions(hash_partitioned_rdd) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give an argument about number of record in each partition\n",
    "\n",
    "- Con of Hash partioning compare to the default strategy (Random Equal Partitioning):\n",
    "\n",
    "Skewed distribution can happen shown in the cell above\n",
    "\n",
    "- Pro of Hash partioning compare to the default strategy (Random Equal Partitioning):\n",
    "\n",
    "Group data in semantical way so if we define a good hash function, it will improve the performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Query RDD  <a class=\"anchor\" id=\"1.3\"></a>\n",
    "### 1.3.1 Collect a total number of flights for each month <a class=\"anchor\" id=\"1.3.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 47136),\n",
       " (2, 42798),\n",
       " (3, 50816),\n",
       " (4, 48810),\n",
       " (5, 49691),\n",
       " (6, 50256),\n",
       " (7, 52065),\n",
       " (8, 50524),\n",
       " (9, 46733),\n",
       " (10, 48680),\n",
       " (11, 46809),\n",
       " (12, 47866)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use map to create tuple that contains month in the first element and 1 in the second (use for count)\n",
    "total_flights_each_month_rdd = flight_rdd.map(lambda x: (x[1],1))\n",
    "\n",
    "# Reduce by key to group each month together and sum 1 acted as count\n",
    "total_flights_each_month_rdd = total_flights_each_month_rdd.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Show the result\n",
    "total_flights_each_month_rdd.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Collect the average delay for each month <a class=\"anchor\" id=\"1.3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5.652155465037339),\n",
       " (2, 7.722627225571288),\n",
       " (3, 4.889286838790932),\n",
       " (4, 3.1355050194632246),\n",
       " (5, 4.644402406874485),\n",
       " (6, 9.534662527857371),\n",
       " (7, 6.701373283395755),\n",
       " (8, 4.652501781331645),\n",
       " (9, -0.8448847709327456),\n",
       " (10, -0.5383935907970419),\n",
       " (11, 0.8206114208805999),\n",
       " (12, 6.035244223457151)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use map to create tuple that contains month in the first element and \n",
    "# tuple of arrival delay and 1 (use for count) in the second \n",
    "average_delay_each_month_rdd = flight_rdd.map(lambda x: (x[1],(x[22],1)))\n",
    "\n",
    "# Reduce by key to group each month together, sum 1 acted as count and sum  total arrival delay\n",
    "average_delay_each_month_rdd = average_delay_each_month_rdd.reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
    "\n",
    "# Calculate average delay using total arrival delay divided by total flights\n",
    "average_delay_each_month_rdd = average_delay_each_month_rdd.mapValues(lambda x : x[0] / x[1])\n",
    "\n",
    "# Show the result\n",
    "average_delay_each_month_rdd.take(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Working with DataFrame <a class=\"anchor\" id=\"2\"></a>\n",
    "## 2.1. Data Preparation and Loading <a class=\"anchor\" id=\"2.1\"></a>\n",
    "### 2.1.1 Define dataframes and loading scheme<a class=\"anchor\" id=\"2.1.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load all flights and airports data into two separate dataframes. Name the dataframes\n",
    "as flightsDf and airportsDf respectively. Hint : use the module\n",
    "spark.read.format(“csv”), with header option is true and inferSchema is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load flight data\n",
    "flightsDf = spark.read.format('csv').option('header',True).option('inferSchema',True).load('data/flight*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load airport data\n",
    "airportsDf = spark.read.format('csv').option('header',True).option('inferSchema',True).load('data/airport*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Display the schema of the final two dataframes<a class=\"anchor\" id=\"2.1.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display the schema of the final of two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display flight schema\n",
    "flightsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- AIRPORT: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display airport schema\n",
    "airportsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Query Analysis <a class=\"anchor\" id=\"2.2\"></a>\n",
    "\n",
    "Implement the following queries using dataframes. You need to be able to perform operations\n",
    "like filtering, sorting, joining and group by using the functions provided by the DataFrame\n",
    "API.\n",
    "\n",
    "### 2.2.1 January flight events with ANC airport <a class=\"anchor\" id=\"2.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Display all the flight events in January 2015 with five columns (Month, Origin\n",
    "Airport, Destination Airport, Distance, and Arrival Delay), where the origin airport\n",
    "'ANC' and name this dataframe as janFlightEventsAncDf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-------------------+--------+-------------+\n",
      "|MONTH|ORIGIN_AIRPORT|DESTINATION_AIRPORT|DISTANCE|ARRIVAL_DELAY|\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "|    1|           ANC|                SEA|    1448|          -13|\n",
      "|    1|           ANC|                SEA|    1448|           -4|\n",
      "|    1|           ANC|                JNU|     571|           17|\n",
      "|    1|           ANC|                CDV|     160|           20|\n",
      "|    1|           ANC|                BET|     399|          -20|\n",
      "|    1|           ANC|                SEA|    1448|          -15|\n",
      "|    1|           ANC|                SEA|    1448|          -11|\n",
      "|    1|           ANC|                ADQ|     253|          -16|\n",
      "|    1|           ANC|                SEA|    1448|           17|\n",
      "|    1|           ANC|                BET|     399|           -9|\n",
      "|    1|           ANC|                SEA|    1448|           15|\n",
      "|    1|           ANC|                FAI|     261|           -6|\n",
      "|    1|           ANC|                JNU|     571|            2|\n",
      "|    1|           ANC|                JNU|     571|           -3|\n",
      "|    1|           ANC|                PDX|    1542|          -21|\n",
      "|    1|           ANC|                SEA|    1448|           -5|\n",
      "|    1|           ANC|                SEA|    1448|          -15|\n",
      "|    1|           ANC|                PDX|    1542|          -13|\n",
      "|    1|           ANC|                SFO|    2018|           20|\n",
      "|    1|           ANC|                FAI|     261|           56|\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the flight event that occurred in month = 1, year = 2015 and the origin airport ANC\n",
    "janFlightEventsAncDf = flightsDf.filter((flightsDf.MONTH ==1) \n",
    "                                        & (flightsDf.YEAR ==2015) \n",
    "                                        & (flightsDf.ORIGIN_AIRPORT == 'ANC'))\n",
    "\n",
    "# Select only five columns (Month, Origin Airport, Destination Airport, Distance, and Arrival Delay)\n",
    "janFlightEventsAncDf = janFlightEventsAncDf.select('MONTH','ORIGIN_AIRPORT','DESTINATION_AIRPORT','DISTANCE','ARRIVAL_DELAY')\n",
    "\n",
    "# Show the result\n",
    "janFlightEventsAncDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Average Arrival Delay From Origin to Destination <a class=\"anchor\" id=\"2.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. From the query results on query no.1, please display a new query. Then please group\n",
    "by ‘ORIGIN_AIRPORT’ AND ‘DESTINATION_AIRPORT’. Add a new column and\n",
    "name it as ‘AVERAGE_DELAY’. This column value is the average from all\n",
    "‘ARRIVAL_DELAY’ values. Then sort it based on ‘AVERAGE_DELAY’. Please\n",
    "name this dataframe as janFlightEventsAncAvgDf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|      AVERAGE_DELAY|\n",
      "+--------------+-------------------+-------------------+\n",
      "|           ANC|                ADK|              -27.0|\n",
      "|           ANC|                HNL|              -20.0|\n",
      "|           ANC|                MSP|             -19.25|\n",
      "|           ANC|                BET| -9.090909090909092|\n",
      "|           ANC|                SEA| -6.490196078431373|\n",
      "|           ANC|                BRW| -4.333333333333333|\n",
      "|           ANC|                OME|               -3.0|\n",
      "|           ANC|                ADQ|-2.6666666666666665|\n",
      "|           ANC|                CDV|                1.0|\n",
      "|           ANC|                OTZ|               1.25|\n",
      "|           ANC|                PHX|                2.0|\n",
      "|           ANC|                DEN| 3.3333333333333335|\n",
      "|           ANC|                PDX|                3.5|\n",
      "|           ANC|                JNU|                5.0|\n",
      "|           ANC|                LAS|                9.0|\n",
      "|           ANC|                SCC| 16.666666666666668|\n",
      "|           ANC|                SFO|               20.0|\n",
      "|           ANC|                FAI|               25.0|\n",
      "+--------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark.sql.functions \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Group by the result by ORIGIN_AIRPORT and DESTINATION_AIRPORT and aggregate average arrival delay\n",
    "janFlightEventsAncAvgDf = janFlightEventsAncDf.groupBy(\n",
    "    F.col('ORIGIN_AIRPORT'),F.col('DESTINATION_AIRPORT')).agg(\n",
    "    F.avg('ARRIVAL_DELAY').alias('AVERAGE_DELAY'))\n",
    "\n",
    "# Sort the result based on average delay\n",
    "janFlightEventsAncAvgDf = janFlightEventsAncAvgDf.sort(janFlightEventsAncAvgDf.AVERAGE_DELAY)\n",
    "\n",
    "# Show the result\n",
    "janFlightEventsAncAvgDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Join Query with Airports DataFrame <a class=\"anchor\" id=\"2.2.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Join the results on query no. 2 janFlightEventsAncAvgDf and airportsDf using inner\n",
    "join operation. You may name this dataset as joinedSqlDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+---------+--------------------+---------+-----+-------+--------+----------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|      AVERAGE_DELAY|IATA_CODE|             AIRPORT|     CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+--------------+-------------------+-------------------+---------+--------------------+---------+-----+-------+--------+----------+\n",
      "|           ANC|                BRW| -4.333333333333333|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                ADK|              -27.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                OME|               -3.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                JNU|                5.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                LAS|                9.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                SCC| 16.666666666666668|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                CDV|                1.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                DEN| 3.3333333333333335|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                OTZ|               1.25|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                SFO|               20.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                FAI|               25.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                ADQ|-2.6666666666666665|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                PDX|                3.5|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                PHX|                2.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                HNL|              -20.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                SEA| -6.490196078431373|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                MSP|             -19.25|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                BET| -9.090909090909092|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "+--------------+-------------------+-------------------+---------+--------------------+---------+-----+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner Join janFlightEventsAncAvgDf and airportsD\n",
    "joinedSqlDf = janFlightEventsAncAvgDf.join(airportsDf,\n",
    "                                           janFlightEventsAncAvgDf.ORIGIN_AIRPORT == airportsDf.IATA_CODE,\n",
    "                                           how ='inner')\n",
    "\n",
    "joinedSqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Analysis <a class=\"anchor\" id=\"2.3\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "In this section, we want to analyse when the delays are most likely to happen using Spark\n",
    "SQL. By obtaining the day of week and month in all history of flight, implement the\n",
    "following queries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Views from Dataframes\n",
    "flightsDf.createOrReplaceTempView(\"sql_flights\")\n",
    "airportsDf.createOrReplaceTempView(\"sql_airports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Relationship between day of week with mean arrival delay, total time delay, and count flights <a class=\"anchor\" id=\"2.3.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "1. Find the total number of flights events, total time delay and average of arrival delay\n",
    "for each day of week (‘DAY_OF_WEEK’) sorted by the value of NumOfFlights in\n",
    "descending order. This query represents the summary of all 2015 flights. What can\n",
    "you analyse from this query results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------+------------+\n",
      "|DAY_OF_WEEK|  MeanArrivalDelay|TotalTimeDelay|NumOfFlights|\n",
      "+-----------+------------------+--------------+------------+\n",
      "|          4| 5.684831897201573|        490186|       87683|\n",
      "|          1| 5.883000999381335|        494478|       86317|\n",
      "|          5| 4.715112525093624|        401638|       86253|\n",
      "|          3|3.9745505431431147|        335150|       85607|\n",
      "|          2| 4.391518272706391|        363262|       84449|\n",
      "|          7| 4.299206488272548|        343498|       81422|\n",
      "|          6| 1.813841449342257|        125750|       70453|\n",
      "+-----------+------------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_task2_3_1 = spark.sql('''\n",
    "  SELECT DAY_OF_WEEK,avg(ARRIVAL_DELAY) as MeanArrivalDelay, sum(ARRIVAL_DELAY) as TotalTimeDelay,\n",
    "  count(FLIGHT_NUMBER) as NumOfFlights\n",
    "  FROM sql_flights\n",
    "  WHERE year = 2015\n",
    "  GROUP BY DAY_OF_WEEK\n",
    "  ORDER BY count(FLIGHT_NUMBER) desc\n",
    "\n",
    "''')\n",
    "\n",
    "sql_task2_3_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Day of week 1 (Monday) and 4(Thursday) have the highest number of flights as well as highest number of total time delay and mean arrival delay, while day of week 6 (Saturday) has the least in every shown measurement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Display mean arrival delay each month <a class=\"anchor\" id=\"2.3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the average of arrival delay, total time delay, and total number of flight events for\n",
    "each month (‘MONTH’) sorted by MeanArrivalDelay in ascending order (default).\n",
    "What can you analyse from this query results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------------+--------------+\n",
      "|MONTH|   MeanArrivalDelay|NumOfFlights|TotalTimeDelay|\n",
      "+-----+-------------------+------------+--------------+\n",
      "|    9|-0.8498676252179341|       46733|        -39484|\n",
      "|   10| -0.541989784312509|       48680|        -26209|\n",
      "|   11| 0.8313745860658399|       46809|         38412|\n",
      "|    4|  3.173803944339603|       48810|        153044|\n",
      "|    5| 4.7121097658084405|       49691|        230785|\n",
      "|    8|  4.713893233866763|       50524|        235063|\n",
      "|    3|  5.011173860427592|       50816|        248454|\n",
      "|    1|  5.804357298474946|       47136|        266420|\n",
      "|   12|   6.15837046195826|       47866|        288883|\n",
      "|    7|  6.786093552465234|       52065|        348907|\n",
      "|    2|  8.123906203913085|       42798|        330513|\n",
      "|    6|  9.747630090727856|       50256|        479174|\n",
      "+-----+-------------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_task2_3_2 = spark.sql('''\n",
    "  SELECT MONTH,avg(ARRIVAL_DELAY) as MeanArrivalDelay,\n",
    "  count(FLIGHT_NUMBER) as NumOfFlights, sum(ARRIVAL_DELAY) as TotalTimeDelay\n",
    "  FROM sql_flights\n",
    "  GROUP BY MONTH\n",
    "  ORDER BY avg(ARRIVAL_DELAY) \n",
    "\n",
    "''')\n",
    "\n",
    "sql_task2_3_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Flights on Month 9(September) and month 10(October) seemed to have earlier schdeduled time than determined arrival time due to the negative value of total time delay and mean arrival delay, while on month 2 (Feburary) and month 6 (June) were the month that had the highest mean arrival delay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Relationship between mean departure delay and mean arrival delay <a class=\"anchor\" id=\"2.3.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Display the mean departure delay (MeanDeptDelay) and mean arrival delay\n",
    "(MeanArrivalDelay) for each month (‘MONTH’) sorted by MeanDeptDelay in\n",
    "descending order. What you can analyse from the relationship between two columns:\n",
    "Mean Departure Delay and Mean Arrival Delay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-------------------+\n",
      "|MONTH|     MeanDeptDelay|   MeanArrivalDelay|\n",
      "+-----+------------------+-------------------+\n",
      "|    6|  13.9730063585922|  9.747630090727856|\n",
      "|   12|11.821651454043728|   6.15837046195826|\n",
      "|    7|11.708608758020432|  6.786093552465234|\n",
      "|    2|11.620796080832823|  8.123906203913085|\n",
      "|    8|10.086906141367324|  4.713893233866763|\n",
      "|    1|  9.75401499511029|  5.804357298474946|\n",
      "|    3| 9.718308159530178|  5.011173860427592|\n",
      "|    5| 9.550310180006102| 4.7121097658084405|\n",
      "|    4| 7.737554783759199|  3.173803944339603|\n",
      "|   11| 6.630585898709037| 0.8313745860658399|\n",
      "|   10| 5.243436261558784| -0.541989784312509|\n",
      "|    9| 4.728506981740065|-0.8498676252179341|\n",
      "+-----+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_task2_3_3 = spark.sql('''\n",
    "  SELECT MONTH,avg(DEPARTURE_DELAY) as MeanDeptDelay,avg(ARRIVAL_DELAY) as MeanArrivalDelay\n",
    "  FROM sql_flights\n",
    "  GROUP BY MONTH\n",
    "  ORDER BY avg(DEPARTURE_DELAY) desc\n",
    "\n",
    "''')\n",
    "\n",
    "sql_task2_3_3.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "The month that had high mean departure delay,especially in month 6(June), seemed to have high mean arrival delay as well. On the contrary, month 9(September) and 10 (October), which had low  mean departure delay seemed to have low arrival delay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 RDDs vs DataFrame vs Spark SQL <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "\n",
    "Implement the following queries using RDDs, DataFrames and SparkSQL separately. Log the time taken for each query in each approach using the “%%time” built-in magic command in Jupyter Notebook and discuss the performance difference of these 3 approaches.\n",
    "\n",
    "<strong>Find the MONTH and DAY_OF_WEEK, number of flights, and average delay where TAIL_NUMBER = ‘N407AS’. Note number of flights and average delay should be aggregated separately. The average delay should be grouped by both MONTH and DAYS_OF_WEEK.</strong>\n",
    "\n",
    "## 3.1 RDD Operation<a class=\"anchor\" id=\"3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column headers are \n",
      "['TAIL_NUMBER', 'MONTH', 'DAY_OF_WEEK', 'NumOfFlights', 'MeanDeptDelay', 'MeanArrivalDelay']\n",
      "CPU times: user 41.2 ms, sys: 4.59 ms, total: 45.8 ms\n",
      "Wall time: 3.41 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['N407AS', 7, 3, 3, -0.3333333333333333, -5.333333333333333],\n",
       " ['N407AS', 12, 2, 2, 1.0, -11.5],\n",
       " ['N407AS', 10, 4, 1, -11.0, -6.0],\n",
       " ['N407AS', 6, 4, 1, -6.0, -15.0],\n",
       " ['N407AS', 8, 5, 3, 1.6666666666666667, -10.0],\n",
       " ['N407AS', 5, 6, 2, 0.5, -3.0],\n",
       " ['N407AS', 2, 3, 2, -12.5, -11.5],\n",
       " ['N407AS', 6, 2, 1, 33.0, 35.0],\n",
       " ['N407AS', 3, 5, 3, 5.666666666666667, 6.666666666666667],\n",
       " ['N407AS', 7, 5, 1, 9.0, -4.0],\n",
       " ['N407AS', 1, 3, 1, -7.0, -27.0],\n",
       " ['N407AS', 10, 5, 3, -6.333333333333333, -3.6666666666666665],\n",
       " ['N407AS', 12, 3, 2, 1.5, 18.5],\n",
       " ['N407AS', 8, 2, 2, -4.0, -11.0],\n",
       " ['N407AS', 11, 7, 3, -5.0, -4.0],\n",
       " ['N407AS', 9, 1, 2, -5.5, -15.5],\n",
       " ['N407AS', 3, 3, 1, 28.0, 3.0],\n",
       " ['N407AS', 1, 5, 2, -6.0, -21.0],\n",
       " ['N407AS', 12, 1, 1, -5.0, -1.0],\n",
       " ['N407AS', 4, 4, 3, 1.6666666666666667, -0.6666666666666666],\n",
       " ['N407AS', 8, 4, 1, -7.0, -5.0],\n",
       " ['N407AS', 5, 7, 3, 4.666666666666667, -7.666666666666667],\n",
       " ['N407AS', 2, 2, 2, -3.5, -9.5],\n",
       " ['N407AS', 11, 1, 1, 57.0, 35.0],\n",
       " ['N407AS', 8, 3, 1, 2.0, -4.0],\n",
       " ['N407AS', 3, 2, 2, -5.5, -28.0],\n",
       " ['N407AS', 5, 1, 3, 2.0, 4.666666666666667],\n",
       " ['N407AS', 2, 4, 2, -8.5, -11.0],\n",
       " ['N407AS', 4, 2, 1, -2.0, 6.0],\n",
       " ['N407AS', 8, 1, 2, -14.0, -13.0],\n",
       " ['N407AS', 1, 6, 3, 8.666666666666666, 4.333333333333333],\n",
       " ['N407AS', 9, 2, 1, 8.0, -10.0],\n",
       " ['N407AS', 11, 4, 2, -7.5, -1.0],\n",
       " ['N407AS', 6, 6, 3, -2.0, -7.666666666666667],\n",
       " ['N407AS', 7, 1, 1, -3.0, -1.0],\n",
       " ['N407AS', 4, 3, 1, -4.0, -7.0],\n",
       " ['N407AS', 10, 1, 2, 12.5, 15.5],\n",
       " ['N407AS', 12, 7, 2, -2.0, -1.0],\n",
       " ['N407AS', 2, 5, 1, -11.0, -31.0],\n",
       " ['N407AS', 7, 7, 5, 15.8, 15.2],\n",
       " ['N407AS', 1, 1, 1, 4.0, -6.0],\n",
       " ['N407AS', 2, 7, 2, -7.0, 6.5],\n",
       " ['N407AS', 5, 2, 5, 6.0, 0.8],\n",
       " ['N407AS', 10, 3, 2, 0.0, 1.0],\n",
       " ['N407AS', 4, 1, 1, -1.0, 0.0],\n",
       " ['N407AS', 12, 5, 2, -4.5, 2.0],\n",
       " ['N407AS', 9, 3, 5, -5.2, -14.6],\n",
       " ['N407AS', 11, 5, 1, -4.0, 12.0],\n",
       " ['N407AS', 3, 1, 1, 40.0, 29.0],\n",
       " ['N407AS', 3, 6, 1, -1.0, -3.0]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Filter the flight event that TAIL_NUMBER = N407AS\n",
    "rdd_task3_1 = flight_rdd.filter(lambda x: x[6] == 'N407AS')\n",
    "\n",
    "# Create tuple that tuple of 'TAIL_NUMBER', 'MONTH', 'DAY_OF_WEEK' acts as a key and 1 (Count the number of flights),\n",
    "# DEPARTURE_DELAY and ARRIVAL_DELAY acts as a value\n",
    "rdd_task3_1 = rdd_task3_1.map(lambda x:((x[6],x[1],x[3]),(1,x[11],x[22])) )\n",
    "\n",
    "# Group by 'TAIL_NUMBER', 'MONTH', 'DAY_OF_WEEK' and sum all values\n",
    "rdd_task3_1 = rdd_task3_1.reduceByKey(lambda x, y: (x[0]+y[0],x[1]+y[1],x[2]+y[2]))\n",
    "\n",
    "# Transform the columns to 'NumOfFlights','MeanDeptDelay','MeanArrivalDelay'\n",
    "rdd_task3_1 = rdd_task3_1.mapValues(lambda x : (x[0],x[1]/x[0],x[2]/x[0]))\n",
    "\n",
    "# Unpack the tuple into list\n",
    "rdd_task3_1 = rdd_task3_1.map(lambda x: list(x[0]) + list(x[1]) )\n",
    "\n",
    "# Print the column headers\n",
    "print('The column headers are ')\n",
    "print([flight_header[6],flight_header[1],flight_header[3],'NumOfFlights','MeanDeptDelay','MeanArrivalDelay'])\n",
    "\n",
    "# Show the result\n",
    "rdd_task3_1.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DataFrame Operation<a class=\"anchor\" id=\"3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------+------------+-------------------+-------------------+\n",
      "|TAIL_NUMBER|MONTH|DAY_OF_WEEK|NumOfFlights|      MeanDeptDelay|   MeanArrivalDelay|\n",
      "+-----------+-----+-----------+------------+-------------------+-------------------+\n",
      "|     N407AS|    1|          1|           1|                4.0|               -6.0|\n",
      "|     N407AS|    1|          2|           2|               12.5|               17.5|\n",
      "|     N407AS|    1|          3|           1|               -7.0|              -27.0|\n",
      "|     N407AS|    1|          5|           2|               -6.0|              -21.0|\n",
      "|     N407AS|    1|          6|           3|  8.666666666666666|  4.333333333333333|\n",
      "|     N407AS|    2|          1|           2|               -4.0|               -2.5|\n",
      "|     N407AS|    2|          2|           2|               -3.5|               -9.5|\n",
      "|     N407AS|    2|          3|           2|              -12.5|              -11.5|\n",
      "|     N407AS|    2|          4|           2|               -8.5|              -11.0|\n",
      "|     N407AS|    2|          5|           1|              -11.0|              -31.0|\n",
      "|     N407AS|    2|          7|           2|               -7.0|                6.5|\n",
      "|     N407AS|    3|          1|           1|               40.0|               29.0|\n",
      "|     N407AS|    3|          2|           2|               -5.5|              -28.0|\n",
      "|     N407AS|    3|          3|           1|               28.0|                3.0|\n",
      "|     N407AS|    3|          4|           1|                1.0|                2.0|\n",
      "|     N407AS|    3|          5|           3|  5.666666666666667|  6.666666666666667|\n",
      "|     N407AS|    3|          6|           1|               -1.0|               -3.0|\n",
      "|     N407AS|    4|          1|           1|               -1.0|                0.0|\n",
      "|     N407AS|    4|          2|           1|               -2.0|                6.0|\n",
      "|     N407AS|    4|          3|           1|               -4.0|               -7.0|\n",
      "|     N407AS|    4|          4|           3| 1.6666666666666667|-0.6666666666666666|\n",
      "|     N407AS|    4|          6|           1|                1.0|              -20.0|\n",
      "|     N407AS|    4|          7|           1|               -8.0|              -22.0|\n",
      "|     N407AS|    5|          1|           3|                2.0|  4.666666666666667|\n",
      "|     N407AS|    5|          2|           5|                6.0|                0.8|\n",
      "|     N407AS|    5|          3|           1|               -6.0|               30.0|\n",
      "|     N407AS|    5|          5|           1|               17.0|                6.0|\n",
      "|     N407AS|    5|          6|           2|                0.5|               -3.0|\n",
      "|     N407AS|    5|          7|           3|  4.666666666666667| -7.666666666666667|\n",
      "|     N407AS|    6|          1|           4|                4.5|                7.0|\n",
      "|     N407AS|    6|          2|           1|               33.0|               35.0|\n",
      "|     N407AS|    6|          3|           3|               -5.0|-10.666666666666666|\n",
      "|     N407AS|    6|          4|           1|               -6.0|              -15.0|\n",
      "|     N407AS|    6|          6|           3|               -2.0| -7.666666666666667|\n",
      "|     N407AS|    7|          1|           1|               -3.0|               -1.0|\n",
      "|     N407AS|    7|          3|           3|-0.3333333333333333| -5.333333333333333|\n",
      "|     N407AS|    7|          4|           2|               -2.0|               -4.0|\n",
      "|     N407AS|    7|          5|           1|                9.0|               -4.0|\n",
      "|     N407AS|    7|          7|           5|               15.8|               19.0|\n",
      "|     N407AS|    8|          1|           2|              -14.0|              -13.0|\n",
      "|     N407AS|    8|          2|           2|               -4.0|              -11.0|\n",
      "|     N407AS|    8|          3|           1|                2.0|               -4.0|\n",
      "|     N407AS|    8|          4|           1|               -7.0|               -5.0|\n",
      "|     N407AS|    8|          5|           3| 1.6666666666666667|              -10.0|\n",
      "|     N407AS|    8|          6|           2|               -2.0|               -5.0|\n",
      "|     N407AS|    8|          7|           2|               66.5|               60.5|\n",
      "|     N407AS|    9|          1|           2|               -5.5|              -15.5|\n",
      "|     N407AS|    9|          2|           1|                8.0|              -10.0|\n",
      "|     N407AS|    9|          3|           5|               -5.2|              -14.6|\n",
      "|     N407AS|    9|          4|           3| -8.333333333333334|-10.666666666666666|\n",
      "+-----------+-----+-----------+------------+-------------------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n",
      "CPU times: user 3.55 ms, sys: 8.81 ms, total: 12.4 ms\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import pyspark.sql.functions \n",
    "# import pyspark.sql.functions as F\n",
    "\n",
    "# Filter the flight event that TAIL_NUMBER = N407AS\n",
    "df_task3_2  = flightsDf.filter(flightsDf.TAIL_NUMBER  == 'N407AS')\n",
    "\n",
    "# Group By 'TAIL_NUMBER', 'MONTH', 'DAY_OF_WEEK' and aggregate all measurement\n",
    "df_task3_2  = df_task3_2.groupBy(F.col('TAIL_NUMBER'),\n",
    "                                 F.col('MONTH'),\n",
    "                                 F.col('DAY_OF_WEEK')).agg(F.count('FLIGHT_NUMBER').alias('NumOfFlights'),\n",
    "                                                           F.avg('DEPARTURE_DELAY').alias('MeanDeptDelay'),\n",
    "                                                          F.avg('ARRIVAL_DELAY').alias('MeanArrivalDelay'))\n",
    "\n",
    "# Sort by MeanArrivalDelay\n",
    "df_task3_2 = df_task3_2.sort(df_task3_2.MONTH,df_task3_2.DAY_OF_WEEK)\n",
    "\n",
    "# Show the result\n",
    "df_task3_2.show(50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Spark SQL OPERATION<a class=\"anchor\" id=\"3.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------+------------+-------------------+-------------------+\n",
      "|TAIL_NUMBER|MONTH|DAY_OF_WEEK|NumOfFlights|      MeanDeptDelay|   MeanArrivalDelay|\n",
      "+-----------+-----+-----------+------------+-------------------+-------------------+\n",
      "|     N407AS|    1|          1|           1|                4.0|               -6.0|\n",
      "|     N407AS|    1|          2|           2|               12.5|               17.5|\n",
      "|     N407AS|    1|          3|           1|               -7.0|              -27.0|\n",
      "|     N407AS|    1|          5|           2|               -6.0|              -21.0|\n",
      "|     N407AS|    1|          6|           3|  8.666666666666666|  4.333333333333333|\n",
      "|     N407AS|    2|          1|           2|               -4.0|               -2.5|\n",
      "|     N407AS|    2|          2|           2|               -3.5|               -9.5|\n",
      "|     N407AS|    2|          3|           2|              -12.5|              -11.5|\n",
      "|     N407AS|    2|          4|           2|               -8.5|              -11.0|\n",
      "|     N407AS|    2|          5|           1|              -11.0|              -31.0|\n",
      "|     N407AS|    2|          7|           2|               -7.0|                6.5|\n",
      "|     N407AS|    3|          1|           1|               40.0|               29.0|\n",
      "|     N407AS|    3|          2|           2|               -5.5|              -28.0|\n",
      "|     N407AS|    3|          3|           1|               28.0|                3.0|\n",
      "|     N407AS|    3|          4|           1|                1.0|                2.0|\n",
      "|     N407AS|    3|          5|           3|  5.666666666666667|  6.666666666666667|\n",
      "|     N407AS|    3|          6|           1|               -1.0|               -3.0|\n",
      "|     N407AS|    4|          1|           1|               -1.0|                0.0|\n",
      "|     N407AS|    4|          2|           1|               -2.0|                6.0|\n",
      "|     N407AS|    4|          3|           1|               -4.0|               -7.0|\n",
      "|     N407AS|    4|          4|           3| 1.6666666666666667|-0.6666666666666666|\n",
      "|     N407AS|    4|          6|           1|                1.0|              -20.0|\n",
      "|     N407AS|    4|          7|           1|               -8.0|              -22.0|\n",
      "|     N407AS|    5|          1|           3|                2.0|  4.666666666666667|\n",
      "|     N407AS|    5|          2|           5|                6.0|                0.8|\n",
      "|     N407AS|    5|          3|           1|               -6.0|               30.0|\n",
      "|     N407AS|    5|          5|           1|               17.0|                6.0|\n",
      "|     N407AS|    5|          6|           2|                0.5|               -3.0|\n",
      "|     N407AS|    5|          7|           3|  4.666666666666667| -7.666666666666667|\n",
      "|     N407AS|    6|          1|           4|                4.5|                7.0|\n",
      "|     N407AS|    6|          2|           1|               33.0|               35.0|\n",
      "|     N407AS|    6|          3|           3|               -5.0|-10.666666666666666|\n",
      "|     N407AS|    6|          4|           1|               -6.0|              -15.0|\n",
      "|     N407AS|    6|          6|           3|               -2.0| -7.666666666666667|\n",
      "|     N407AS|    7|          1|           1|               -3.0|               -1.0|\n",
      "|     N407AS|    7|          3|           3|-0.3333333333333333| -5.333333333333333|\n",
      "|     N407AS|    7|          4|           2|               -2.0|               -4.0|\n",
      "|     N407AS|    7|          5|           1|                9.0|               -4.0|\n",
      "|     N407AS|    7|          7|           5|               15.8|               19.0|\n",
      "|     N407AS|    8|          1|           2|              -14.0|              -13.0|\n",
      "|     N407AS|    8|          2|           2|               -4.0|              -11.0|\n",
      "|     N407AS|    8|          3|           1|                2.0|               -4.0|\n",
      "|     N407AS|    8|          4|           1|               -7.0|               -5.0|\n",
      "|     N407AS|    8|          5|           3| 1.6666666666666667|              -10.0|\n",
      "|     N407AS|    8|          6|           2|               -2.0|               -5.0|\n",
      "|     N407AS|    8|          7|           2|               66.5|               60.5|\n",
      "|     N407AS|    9|          1|           2|               -5.5|              -15.5|\n",
      "|     N407AS|    9|          2|           1|                8.0|              -10.0|\n",
      "|     N407AS|    9|          3|           5|               -5.2|              -14.6|\n",
      "|     N407AS|    9|          4|           3| -8.333333333333334|-10.666666666666666|\n",
      "+-----------+-----+-----------+------------+-------------------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 2.67 ms, total: 2.67 ms\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Write the SQL command\n",
    "sql_task3_3 = spark.sql('''\n",
    "  SELECT TAIL_NUMBER,MONTH,DAY_OF_WEEK,\n",
    "  count(FLIGHT_NUMBER) as NumOfFlights,\n",
    "  avg(DEPARTURE_DELAY) as MeanDeptDelay, avg(ARRIVAL_DELAY) as MeanArrivalDelay\n",
    "  FROM sql_flights\n",
    "  WHERE TAIL_NUMBER = 'N407AS'\n",
    "  GROUP BY TAIL_NUMBER,MONTH,DAY_OF_WEEK\n",
    "  ORDER BY MONTH,DAY_OF_WEEK\n",
    "\n",
    "''')\n",
    "\n",
    "# Show the result\n",
    "sql_task3_3.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Discussion<a class=\"anchor\" id=\"3.4\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the %%time used for experiment above, it can be found that Spark SQL used the least total CPU times followed by dataframe operation and RDD consumed the longest CPU times. The link below gives us comparison between RDD, dataframe and SQL saying DF performs the best following by SQL and RDD performs worst\n",
    "\n",
    "Reference\n",
    "https://www.analyticsvidhya.com/blog/2020/11/what-is-the-difference-between-rdds-dataframes-and-datasets/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5202 Assignment 1 SOLUTION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
